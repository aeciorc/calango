from lexer.tokenizer import tokenize

result_tokens = tokenize('lexer/test_data')
print(result_tokens)

